{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8237211,"sourceType":"datasetVersion","datasetId":4885061}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Facial prediction: fine tuning and an attempt using wandb","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for i,filename in enumerate(filenames):\n        print(os.path.join(dirname, filename))\n        if i>=10:\n            break\n        \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-14T22:06:58.897Z","iopub.execute_input":"2024-07-14T22:06:58.89728Z","iopub.status.idle":"2024-07-14T22:07:02.306749Z","shell.execute_reply.started":"2024-07-14T22:06:58.897254Z","shell.execute_reply":"2024-07-14T22:07:02.305714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo ... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:02.307974Z","iopub.execute_input":"2024-07-14T22:07:02.308364Z","iopub.status.idle":"2024-07-14T22:07:04.137055Z","shell.execute_reply.started":"2024-07-14T22:07:02.308336Z","shell.execute_reply":"2024-07-14T22:07:04.136034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Content of the notebook\n1. Beauty rate using pretrained model ViT on hugging face Transformers\n2. denoise, refine the photo\n3. generate ID photo","metadata":{}},{"cell_type":"markdown","source":"##  Import dataset, preprocessing, Composing dataloader","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\nBASE_DIR = \"/kaggle/input/scut-fbp5500-v2-facial-beauty-scores\"\ndata=[]\n\nwith open(f'{BASE_DIR}/labels.txt', 'r',encoding='utf-8') as labels_file:\n    labels = labels_file.readlines()\n#     print(labels)\n    for label in tqdm(labels):\n        row = label.rstrip('\\n').split(' ')\n        data.append(row)\n        \ndf=pd.DataFrame(data, columns =['filename','beauty_rate'])","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:04.138268Z","iopub.execute_input":"2024-07-14T22:07:04.138654Z","iopub.status.idle":"2024-07-14T22:07:04.237041Z","shell.execute_reply.started":"2024-07-14T22:07:04.138614Z","shell.execute_reply":"2024-07-14T22:07:04.23631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nIMAGE_DIR = BASE_DIR +'/Images/Images/'\n\nimage_tensor = torchvision.io.read_image(IMAGE_DIR+df.iloc[1,0])\nimage_tensor.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:04.240235Z","iopub.execute_input":"2024-07-14T22:07:04.240901Z","iopub.status.idle":"2024-07-14T22:07:05.25465Z","shell.execute_reply.started":"2024-07-14T22:07:04.240863Z","shell.execute_reply":"2024-07-14T22:07:05.253692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## sample a few images to show\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport math\nimport torch\n\n\ndef plot_beauty(df, img_dir, num=5 ,random_seed=42):\n    fig = plt.figure(figsize=(9,9))\n    rows, cols = math.ceil(num/3) ,3\n    torch.manual_seed(random_seed)\n    for i in range(num):\n        id = torch.randint(0,len(df),size=[1]).item()\n        img_path, label = f'{img_dir}/{df.iloc[id,0]}', df.iloc[id,1]\n        fig.add_subplot(rows, cols, i+1)\n        im = Image.open(img_path)\n        plt.imshow(im)\n        plt.title('beauty label:'+label)\n        plt.axis(False)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:05.255793Z","iopub.execute_input":"2024-07-14T22:07:05.256182Z","iopub.status.idle":"2024-07-14T22:07:05.263707Z","shell.execute_reply.started":"2024-07-14T22:07:05.256157Z","shell.execute_reply":"2024-07-14T22:07:05.262738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_beauty(df, IMAGE_DIR, random_seed=68)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:05.2651Z","iopub.execute_input":"2024-07-14T22:07:05.265789Z","iopub.status.idle":"2024-07-14T22:07:05.954425Z","shell.execute_reply.started":"2024-07-14T22:07:05.265754Z","shell.execute_reply":"2024-07-14T22:07:05.95349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader, Subset\nimport os\n\n\nclass FaceData(Dataset):\n    def __init__(self, df, img_dir, transform):\n        self.df = df\n        self.img_dir = img_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        filename, label = self.df.iloc[idx].values\n        img_path = os.path.join(self.img_dir, filename)\n        \n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        \n        return image, torch.tensor(float(label), dtype=torch.float32), img_path\n    \n#     def get_path_by_idx(self, idx):\n#         filename, label = self.df.iloc[idx].values\n#         img_path = os.path.join(self.img_dir, filename)\n#         return img_path\n        ","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:05.955611Z","iopub.execute_input":"2024-07-14T22:07:05.955918Z","iopub.status.idle":"2024-07-14T22:07:05.963692Z","shell.execute_reply.started":"2024-07-14T22:07:05.955893Z","shell.execute_reply":"2024-07-14T22:07:05.962608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torchvision import transforms\n\n# transform1 = transforms.Compose([\n#     transforms.Resize((128,128)),\n#     transforms.ToTensor()\n# ])\n\n# faceds = FaceData(df, f'{BASE_DIR}/Images/Images', transform = transform1)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:05.964863Z","iopub.execute_input":"2024-07-14T22:07:05.965119Z","iopub.status.idle":"2024-07-14T22:07:05.973102Z","shell.execute_reply.started":"2024-07-14T22:07:05.965088Z","shell.execute_reply":"2024-07-14T22:07:05.972267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#### 2. train / test split\n\n# val_size = 0.2\n# indices = list(range(len(df)))\n\n# np.random.shuffle(indices)\n# split = int(np.floor(val_size * len(df)))\n# train_indices, val_indices = indices[split:], indices[:split]\n\n# train_ds = Subset(faceds, train_indices)\n# val_ds = Subset(faceds, val_indices)\n\n# train_loader = DataLoader(train_ds, batch_size=32, shuffle = True)\n# val_loader = DataLoader(val_ds, batch_size=32, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:05.97428Z","iopub.execute_input":"2024-07-14T22:07:05.97498Z","iopub.status.idle":"2024-07-14T22:07:05.982532Z","shell.execute_reply.started":"2024-07-14T22:07:05.974946Z","shell.execute_reply":"2024-07-14T22:07:05.981626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using the keypoint detection model from pytorch\nhttps://pytorch.org/vision/stable/models.html#keypoint-detection","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.models.detection import keypointrcnn_resnet50_fpn, KeypointRCNN_ResNet50_FPN_Weights\nfrom torchvision.io import read_image\n\nperson_int = read_image(IMAGE_DIR+df.iloc[8,0])\n\nweights = KeypointRCNN_ResNet50_FPN_Weights.DEFAULT\ntransforms = weights.transforms()\n\nperson_float = transforms(person_int)\n\nmodel = keypointrcnn_resnet50_fpn(weights=weights, progress=False)\nmodel = model.eval()\n\noutputs = model([person_float])\nprint(outputs)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:05.983536Z","iopub.execute_input":"2024-07-14T22:07:05.984081Z","iopub.status.idle":"2024-07-14T22:07:09.663846Z","shell.execute_reply.started":"2024-07-14T22:07:05.984056Z","shell.execute_reply":"2024-07-14T22:07:09.662717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kpts = outputs[0]['keypoints']\nscores = outputs[0]['scores']\n\nprint(kpts)\nprint(scores)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:09.665325Z","iopub.execute_input":"2024-07-14T22:07:09.665868Z","iopub.status.idle":"2024-07-14T22:07:09.672929Z","shell.execute_reply.started":"2024-07-14T22:07:09.665838Z","shell.execute_reply":"2024-07-14T22:07:09.671978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms.functional as F\n\ndef show(imgs):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:09.674008Z","iopub.execute_input":"2024-07-14T22:07:09.674293Z","iopub.status.idle":"2024-07-14T22:07:09.681485Z","shell.execute_reply.started":"2024-07-14T22:07:09.674265Z","shell.execute_reply":"2024-07-14T22:07:09.68065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detect_threshold = 0.75\nidx = torch.where(scores > detect_threshold)\nkeypoints = kpts[idx]","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:09.68742Z","iopub.execute_input":"2024-07-14T22:07:09.68795Z","iopub.status.idle":"2024-07-14T22:07:09.692661Z","shell.execute_reply.started":"2024-07-14T22:07:09.687921Z","shell.execute_reply":"2024-07-14T22:07:09.69175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.utils import draw_keypoints\n\nres = draw_keypoints(person_int, keypoints, colors=\"blue\",radius=3)\nshow(res)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:09.69389Z","iopub.execute_input":"2024-07-14T22:07:09.694166Z","iopub.status.idle":"2024-07-14T22:07:09.939754Z","shell.execute_reply.started":"2024-07-14T22:07:09.694143Z","shell.execute_reply":"2024-07-14T22:07:09.938809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dir(model)\n# model.state_dict\n# model","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:09.940961Z","iopub.execute_input":"2024-07-14T22:07:09.941247Z","iopub.status.idle":"2024-07-14T22:07:09.945227Z","shell.execute_reply.started":"2024-07-14T22:07:09.941222Z","shell.execute_reply":"2024-07-14T22:07:09.944144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print a summary using torchinfo (uncomment for actual output)\nsummary(model=model, \n        input_size=(32, 3,128,128), # make sure this is \"input_size\", not \"input_shape\"\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n) ","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:09.946351Z","iopub.execute_input":"2024-07-14T22:07:09.946692Z","iopub.status.idle":"2024-07-14T22:07:11.936588Z","shell.execute_reply.started":"2024-07-14T22:07:09.946659Z","shell.execute_reply":"2024-07-14T22:07:11.935687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use pretrained model on transformers ViT, and finetune to do image classification","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:11.937851Z","iopub.execute_input":"2024-07-14T22:07:11.938208Z","iopub.status.idle":"2024-07-14T22:07:24.435845Z","shell.execute_reply.started":"2024-07-14T22:07:11.938175Z","shell.execute_reply":"2024-07-14T22:07:24.43469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom transformers import AutoImageProcessor, ViTForImageClassification, ViTFeatureExtractor\n\nmodelVit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\nimage_processor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224')\n\n\n# Step 2: Modify the Model Architecture\nclass ModifiedViT(nn.Module):\n    def __init__(self, base_model):\n        super(ModifiedViT, self).__init__()\n        self.base_model = base_model\n        self.fc1 = nn.Linear(base_model.config.hidden_size, 256)\n        self.dropout1 = nn.Dropout(0.5)\n        self.classifier = nn.Linear(256, 1)\n\n    def forward(self, x):\n        x = self.base_model.vit(x).last_hidden_state[:, 0]\n        x = self.fc1(x)\n        x = nn.ReLU()(x)\n        x = self.dropout1(x)\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:24.437363Z","iopub.execute_input":"2024-07-14T22:07:24.437709Z","iopub.status.idle":"2024-07-14T22:07:29.062001Z","shell.execute_reply.started":"2024-07-14T22:07:24.437676Z","shell.execute_reply":"2024-07-14T22:07:29.061067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Make full use of GPUs Aavailable, since we have 2 T4 to use.\n\nmodifiedVit = ModifiedViT(modelVit)\n\nprint(\"Number of GPUs available:\", torch.cuda.device_count())\n\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n    ModifiedViT = nn.DataParallel(ModifiedViT)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:29.063341Z","iopub.execute_input":"2024-07-14T22:07:29.064275Z","iopub.status.idle":"2024-07-14T22:07:29.072658Z","shell.execute_reply.started":"2024-07-14T22:07:29.064238Z","shell.execute_reply":"2024-07-14T22:07:29.071523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## freeze the parameters of pretrained ViT\n\nmodifiedVit.base_model.classifier = None\n\nfor param in modifiedVit.base_model.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:29.07385Z","iopub.execute_input":"2024-07-14T22:07:29.074115Z","iopub.status.idle":"2024-07-14T22:07:29.085538Z","shell.execute_reply.started":"2024-07-14T22:07:29.074092Z","shell.execute_reply":"2024-07-14T22:07:29.084691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modifiedVit\nsummary(modifiedVit)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:29.086579Z","iopub.execute_input":"2024-07-14T22:07:29.086943Z","iopub.status.idle":"2024-07-14T22:07:29.151474Z","shell.execute_reply.started":"2024-07-14T22:07:29.086918Z","shell.execute_reply":"2024-07-14T22:07:29.150605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_processor","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:29.152625Z","iopub.execute_input":"2024-07-14T22:07:29.15298Z","iopub.status.idle":"2024-07-14T22:07:29.158762Z","shell.execute_reply.started":"2024-07-14T22:07:29.152947Z","shell.execute_reply":"2024-07-14T22:07:29.157869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## construct transformation for images\n\nfrom torchvision.transforms import (\n    CenterCrop,\n    Compose,\n    Normalize,\n    RandomHorizontalFlip,\n    RandomResizedCrop,\n    Resize,\n    ToTensor,\n)\n\nnormalize =Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\nif \"height\" in image_processor.size:\n    size=(image_processor.size[\"height\"],image_processor.size[\"width\"])\n    \n    crop_size = size\n    max_size =None\n    \n\ntrain_transforms = Compose(\n    [\n        RandomResizedCrop(crop_size),\n        RandomHorizontalFlip(p=0.4),\n        ToTensor(),\n        normalize,\n    ]\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:29.159824Z","iopub.execute_input":"2024-07-14T22:07:29.160057Z","iopub.status.idle":"2024-07-14T22:07:29.167127Z","shell.execute_reply.started":"2024-07-14T22:07:29.160037Z","shell.execute_reply":"2024-07-14T22:07:29.16617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_transforms = Compose(\n    [\n        Resize(size),\n        CenterCrop(crop_size),\n        ToTensor(),\n        normalize,\n    ]\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:29.168293Z","iopub.execute_input":"2024-07-14T22:07:29.168586Z","iopub.status.idle":"2024-07-14T22:07:29.179428Z","shell.execute_reply.started":"2024-07-14T22:07:29.168531Z","shell.execute_reply":"2024-07-14T22:07:29.178605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create train and test dataset and dataloader \n\n\nval_size = 0.2\nindices = list(range(len(df)))\n\nnp.random.shuffle(indices)\nsplit = int(np.floor(val_size * len(df)))\ntrain_indices, val_indices = indices[split:], indices[:split]\n\n\n# train_ds = Subset(faceds, train_indices)\n# val_ds = Subset(faceds, val_indices)\n\ntrain_ds = FaceData(df.iloc[train_indices,:], f'{BASE_DIR}/Images/Images', transform = train_transforms)\nval_ds = FaceData(df.iloc[val_indices,:], f'{BASE_DIR}/Images/Images', transform = val_transforms)\n\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle = True)\nval_loader = DataLoader(val_ds, batch_size=32, shuffle = True, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:29.180479Z","iopub.execute_input":"2024-07-14T22:07:29.180791Z","iopub.status.idle":"2024-07-14T22:07:29.192718Z","shell.execute_reply.started":"2024-07-14T22:07:29.180768Z","shell.execute_reply":"2024-07-14T22:07:29.191952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(df.iloc[train_indices,:])\nprint( f'{BASE_DIR}/Images/Images')","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:29.193882Z","iopub.execute_input":"2024-07-14T22:07:29.194618Z","iopub.status.idle":"2024-07-14T22:07:29.203238Z","shell.execute_reply.started":"2024-07-14T22:07:29.194582Z","shell.execute_reply":"2024-07-14T22:07:29.202265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Use weight and bias to monitor and record the experiments\ntry:\n    import wandb\nexcept:\n    print(\"[INFO] Couldn't find wandb, ... installing it.\")\n    !pip install wandb\n    import wandb\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:29.20423Z","iopub.execute_input":"2024-07-14T22:07:29.206048Z","iopub.status.idle":"2024-07-14T22:07:29.511617Z","shell.execute_reply.started":"2024-07-14T22:07:29.206017Z","shell.execute_reply":"2024-07-14T22:07:29.510874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nkaggle_secret = user_secrets.get_secret(\"wandb_api_key\") \nwandb.login(key=kaggle_secret)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:29.512781Z","iopub.execute_input":"2024-07-14T22:07:29.513112Z","iopub.status.idle":"2024-07-14T22:07:31.824743Z","shell.execute_reply.started":"2024-07-14T22:07:29.513079Z","shell.execute_reply":"2024-07-14T22:07:31.823835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import wandb\n# import random\n\n# # start a new wandb run to track this script\n# wandb.init(\n#     # set the wandb project where this run will be logged\n#     project=\"facial_beauty\",\n#     settings=wandb.Settings(start_method=\"fork\"),\n# #     id =\"facial-240706\",\n# #     resume = \"must\",\n# )","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:31.826032Z","iopub.execute_input":"2024-07-14T22:07:31.826644Z","iopub.status.idle":"2024-07-14T22:07:31.830777Z","shell.execute_reply.started":"2024-07-14T22:07:31.826598Z","shell.execute_reply":"2024-07-14T22:07:31.829827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Log hyperparameters\n# wandb.config = {\n#     \"learning_rate\": 0.001,\n#     \"epochs\": 5,\n#     \"batch_size\": 32\n# }\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:31.831884Z","iopub.execute_input":"2024-07-14T22:07:31.832144Z","iopub.status.idle":"2024-07-14T22:07:31.844488Z","shell.execute_reply.started":"2024-07-14T22:07:31.832116Z","shell.execute_reply":"2024-07-14T22:07:31.843668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define hyperparameter space\nepochs_options = [5, 8]\nlearning_rate_options = [0.0005]\noptimizer_options = ['adam', 'sgd']","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:31.845516Z","iopub.execute_input":"2024-07-14T22:07:31.845788Z","iopub.status.idle":"2024-07-14T22:07:31.853648Z","shell.execute_reply.started":"2024-07-14T22:07:31.845764Z","shell.execute_reply":"2024-07-14T22:07:31.852683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def initialize_model():\n    modelVit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n    modifiedVit_ins = ModifiedViT(modelVit)\n\n    print(\"Number of GPUs available:\", torch.cuda.device_count())\n\n    if torch.cuda.device_count() > 1:\n        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n        modifiedViT_ins = nn.DataParallel(modifiedViT_ins)\n    ## freeze the parameters of pretrained ViT\n\n    modifiedVit_ins.base_model.classifier = None\n\n    for param in modifiedVit_ins.base_model.parameters():\n        param.requires_grad = False\n        \n    print(summary(modifiedVit_ins))\n    \n    return modifiedVit_ins","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:15:20.176127Z","iopub.execute_input":"2024-07-14T22:15:20.176503Z","iopub.status.idle":"2024-07-14T22:15:20.183247Z","shell.execute_reply.started":"2024-07-14T22:15:20.176472Z","shell.execute_reply":"2024-07-14T22:15:20.182215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 4: Fine-Tune the Model\n\n\ndef train_model(num_epochs, learning_rate, optimizer):\n    # Start a new wandb run for each combination\n    wandb.init(project=\"facial_beauty\", id=f\"config_{num_epochs}_{optimizer}\",\n                config={\"epochs\": num_epochs, \"optimizer\": optimizer})\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            \n\n    criterion = nn.MSELoss()\n    modifiedVit1 = initialize_model()\n    modifiedVit1.to(device)\n    if optimizer=='adam':\n        optimizer = optim.Adam(modifiedVit1.parameters(), lr=learning_rate)\n    else:\n        optimizer = optim.SGD(modifiedVit1.parameters(), lr=learning_rate)\n        \n    for epoch in range(num_epochs):\n        modifiedVit1.train()\n        running_loss = 0.0\n        for images, labels, _ in train_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = modifiedVit1(images)\n            loss = criterion(outputs.squeeze(), labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * images.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}')\n\n        # Log the loss\n        wandb.log({\"train_loss\": running_loss / len(train_loader.dataset)})\n\n\n        modifiedVit1.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for images, labels, _ in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = modifiedVit1(images)\n                loss = criterion(outputs.squeeze(), labels)\n                val_loss += loss.item() * images.size(0)\n\n        val_epoch_loss = val_loss / len(val_loader.dataset)\n        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_epoch_loss:.4f}')\n        # Log the loss\n        wandb.log({\"val_loss\": val_loss / len(val_loader.dataset)})\n        \n        # Finish the run\n    wandb.finish()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:23:07.015271Z","iopub.execute_input":"2024-07-14T22:23:07.015932Z","iopub.status.idle":"2024-07-14T22:23:07.028109Z","shell.execute_reply.started":"2024-07-14T22:23:07.015898Z","shell.execute_reply":"2024-07-14T22:23:07.027023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:22:59.074725Z","iopub.execute_input":"2024-07-14T22:22:59.075475Z","iopub.status.idle":"2024-07-14T22:22:59.080003Z","shell.execute_reply.started":"2024-07-14T22:22:59.075441Z","shell.execute_reply":"2024-07-14T22:22:59.078954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"for epochs in epochs_options:\n    for learning_rate in learning_rate_options:\n        for optimizer_choice in optimizer_options:\n            \n            # Train the model\n            train_model(epochs, learning_rate, optimizer_choice)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:23:09.745061Z","iopub.execute_input":"2024-07-14T22:23:09.745906Z","iopub.status.idle":"2024-07-14T22:47:34.231284Z","shell.execute_reply.started":"2024-07-14T22:23:09.745871Z","shell.execute_reply":"2024-07-14T22:47:34.230506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodifiedVit.eval()\n\nindex = 0\n# Get the image and label\nimages, labels, _ = next(iter(val_loader))\nimage = images[index]\nlabel = labels[index]\n\n# Add a batch dimension (required for the model)\nimage = image.unsqueeze(0)\n\n# Move the image and the model to the same device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimage = image.to(device)\nmodel = model.to(device)\n\n# Make the prediction\nwith torch.no_grad():\n    output = modifiedVit(image)\n# If the output is a tensor, convert it to a list/float\npredicted_value = output.item()\n\n# Display the image and the prediction\nimage = image.squeeze().cpu().numpy().transpose((1, 2, 0))\nplt.imshow(image)\nplt.title(f\"Predicted: {predicted_value:.4f}, Actual: {label.item():.4f}\")\nplt.axis('off')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:49.85104Z","iopub.status.idle":"2024-07-14T22:07:49.851397Z","shell.execute_reply.started":"2024-07-14T22:07:49.851222Z","shell.execute_reply":"2024-07-14T22:07:49.851238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculate the predictions for the validation set, and plot the most mistaken predictions, to dig out reasons.","metadata":{}},{"cell_type":"code","source":"val_pred_lst1, val_pred_lst2, val_pred_lst3 = [],[],[]\n\n\nfor i in tqdm(range(len(val_loader))):\n    images, labels, paths = next(iter(val_loader))\n#     print(labels)\n#     print(paths)\n\n    # Move the image and the model to the same device (CPU or GPU)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    images = images.to(device)\n    model = model.to(device)\n\n    # Make the prediction\n    with torch.no_grad():\n        outputs = modifiedVit(images)\n\n        outputs = torch.transpose(outputs, 0,1)\n#         print(outputs)\n\n        outputs = outputs.squeeze().to('cpu')\n    val_pred_lst1 += list(paths)\n    val_pred_lst2 += labels.tolist()\n    val_pred_lst3 += outputs.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:49.852992Z","iopub.status.idle":"2024-07-14T22:07:49.853375Z","shell.execute_reply.started":"2024-07-14T22:07:49.853194Z","shell.execute_reply":"2024-07-14T22:07:49.853211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_pred = pd.DataFrame({\n    'img_path': val_pred_lst1,\n    'label': val_pred_lst2,\n    'pred': val_pred_lst3\n})","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:49.854918Z","iopub.status.idle":"2024-07-14T22:07:49.855626Z","shell.execute_reply.started":"2024-07-14T22:07:49.855355Z","shell.execute_reply":"2024-07-14T22:07:49.855375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_pred.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:49.857177Z","iopub.status.idle":"2024-07-14T22:07:49.858071Z","shell.execute_reply.started":"2024-07-14T22:07:49.85781Z","shell.execute_reply":"2024-07-14T22:07:49.857832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_by_err(df, num_plots):\n    # Calculate the absolute difference between Column2 and Column3\n    df['AbsDiff'] = (df['label'] - df['pred']).abs()\n\n    # Sort the DataFrame by the absolute difference in descending order\n    df_sorted = df.sort_values(by='AbsDiff', ascending=False)\n\n    # Select the top rows\n    top_ = df_sorted.head(num_plots)\n    print(top_)\n    \n    fig = plt.figure(figsize=(9,9))\n    rows, cols = math.ceil(num_plots/3) ,3\n#     torch.manual_seed(random_seed)\n    for i in range(num_plots):\n        top_.iloc[i].values\n        img_path, label, pred,_ =  top_.iloc[i].values\n        fig.add_subplot(rows, cols, i+1)\n        im = Image.open(img_path)\n        plt.imshow(im)\n        plt.title(f'label:{label:.4f}|pred:{pred:.4f}')\n        plt.axis(False)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:49.859159Z","iopub.status.idle":"2024-07-14T22:07:49.8596Z","shell.execute_reply.started":"2024-07-14T22:07:49.859366Z","shell.execute_reply":"2024-07-14T22:07:49.859385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_by_err(val_pred, 18)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:49.861374Z","iopub.status.idle":"2024-07-14T22:07:49.861752Z","shell.execute_reply.started":"2024-07-14T22:07:49.861547Z","shell.execute_reply":"2024-07-14T22:07:49.86157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Widget to predict arbitrary photo!","metadata":{}},{"cell_type":"code","source":"\nimport torch\nfrom torchvision import transforms, models\nfrom IPython.display import display, clear_output\nfrom ipywidgets import widgets, VBox, Output\nfrom PIL import Image\nimport io\nimport matplotlib.pyplot as plt\n\n\ndef preprocess_image(image):\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    image = Image.open(io.BytesIO(image)).convert(\"RGB\")\n    image = transform(image).unsqueeze(0)\n    return image.to(device)\n\ndef predict_beauty_score(image):\n    image = preprocess_image(image)\n    model.eval()\n    with torch.no_grad():\n        output = model(image)\n        score = output.item()\n    return score\n\ndef upload_and_predict():\n    uploader = widgets.FileUpload(accept='image/*', multiple=False)\n    result_button = widgets.Button(description=\"결과 보기\")\n    output = Output()\n\n    def on_button_click(b):\n        for filename in uploader.value:\n            content = uploader.value[filename]['content']\n            uploaded_image = content\n            score = predict_beauty_score(uploaded_image)\n            img = Image.open(io.BytesIO(uploaded_image))\n\n            with output:\n                clear_output(wait=True)\n                plt.figure(figsize=(6, 6))\n                plt.imshow(img)\n                plt.title(f'Beauty Score: {score:.2f}')\n                plt.axis('off')\n                plt.show()\n\n    result_button.on_click(on_button_click)\n    display(VBox([uploader, result_button, output]))\n\nupload_and_predict()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:07:49.862759Z","iopub.status.idle":"2024-07-14T22:07:49.863067Z","shell.execute_reply.started":"2024-07-14T22:07:49.862914Z","shell.execute_reply":"2024-07-14T22:07:49.862927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}