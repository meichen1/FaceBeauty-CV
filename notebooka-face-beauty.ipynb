{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8237211,"sourceType":"datasetVersion","datasetId":4885061}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Facial prediction: fine tuning and an attempt using wandb","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for i,filename in enumerate(filenames):\n        print(os.path.join(dirname, filename))\n        if i>=10:\n            break\n        \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-17T15:07:30.2321Z","iopub.execute_input":"2024-07-17T15:07:30.232623Z","iopub.status.idle":"2024-07-17T15:07:40.092317Z","shell.execute_reply.started":"2024-07-17T15:07:30.23257Z","shell.execute_reply":"2024-07-17T15:07:40.091301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo ... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:40.09426Z","iopub.execute_input":"2024-07-17T15:07:40.094652Z","iopub.status.idle":"2024-07-17T15:07:43.409482Z","shell.execute_reply.started":"2024-07-17T15:07:40.09462Z","shell.execute_reply":"2024-07-17T15:07:43.408503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Content of the notebook\n1. Beauty rate using pretrained model ViT on hugging face Transformers\n2. denoise, refine the photo\n3. generate ID photo","metadata":{}},{"cell_type":"markdown","source":"##  Import dataset, preprocessing, Composing dataloader","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\nBASE_DIR = \"/kaggle/input/scut-fbp5500-v2-facial-beauty-scores\"\ndata=[]\n\nwith open(f'{BASE_DIR}/labels.txt', 'r',encoding='utf-8') as labels_file:\n    labels = labels_file.readlines()\n#     print(labels)\n    for label in tqdm(labels):\n        row = label.rstrip('\\n').split(' ')\n        data.append(row)\n        \ndf=pd.DataFrame(data, columns =['filename','beauty_rate'])","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:43.410882Z","iopub.execute_input":"2024-07-17T15:07:43.411694Z","iopub.status.idle":"2024-07-17T15:07:43.539079Z","shell.execute_reply.started":"2024-07-17T15:07:43.411642Z","shell.execute_reply":"2024-07-17T15:07:43.538178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nIMAGE_DIR = BASE_DIR +'/Images/Images/'\n\nimage_tensor = torchvision.io.read_image(IMAGE_DIR+df.iloc[1,0])\nimage_tensor.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:43.540172Z","iopub.execute_input":"2024-07-17T15:07:43.54043Z","iopub.status.idle":"2024-07-17T15:07:45.068753Z","shell.execute_reply.started":"2024-07-17T15:07:43.540408Z","shell.execute_reply":"2024-07-17T15:07:45.067846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## sample a few images to show\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport math\nimport torch\n\n\ndef plot_beauty(df, img_dir, num=5 ,random_seed=42):\n    fig = plt.figure(figsize=(9,9))\n    rows, cols = math.ceil(num/3) ,3\n    torch.manual_seed(random_seed)\n    for i in range(num):\n        id = torch.randint(0,len(df),size=[1]).item()\n        img_path, label = f'{img_dir}/{df.iloc[id,0]}', df.iloc[id,1]\n        fig.add_subplot(rows, cols, i+1)\n        im = Image.open(img_path)\n        plt.imshow(im)\n        plt.title('beauty label:'+label)\n        plt.axis(False)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:45.071331Z","iopub.execute_input":"2024-07-17T15:07:45.071782Z","iopub.status.idle":"2024-07-17T15:07:45.078928Z","shell.execute_reply.started":"2024-07-17T15:07:45.071754Z","shell.execute_reply":"2024-07-17T15:07:45.07786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_beauty(df, IMAGE_DIR, random_seed=68)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:45.079953Z","iopub.execute_input":"2024-07-17T15:07:45.080237Z","iopub.status.idle":"2024-07-17T15:07:45.882449Z","shell.execute_reply.started":"2024-07-17T15:07:45.080205Z","shell.execute_reply":"2024-07-17T15:07:45.881482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader, Subset\nimport os\n\n\nclass FaceData(Dataset):\n    def __init__(self, df, img_dir, transform):\n        self.df = df\n        self.img_dir = img_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        filename, label = self.df.iloc[idx].values\n        img_path = os.path.join(self.img_dir, filename)\n        \n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        \n        return image, torch.tensor(float(label), dtype=torch.float32), img_path\n    \n#     def get_path_by_idx(self, idx):\n#         filename, label = self.df.iloc[idx].values\n#         img_path = os.path.join(self.img_dir, filename)\n#         return img_path\n        ","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:45.883747Z","iopub.execute_input":"2024-07-17T15:07:45.884391Z","iopub.status.idle":"2024-07-17T15:07:45.893467Z","shell.execute_reply.started":"2024-07-17T15:07:45.884355Z","shell.execute_reply":"2024-07-17T15:07:45.892641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torchvision import transforms\n\n# transform1 = transforms.Compose([\n#     transforms.Resize((128,128)),\n#     transforms.ToTensor()\n# ])\n\n# faceds = FaceData(df, f'{BASE_DIR}/Images/Images', transform = transform1)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:45.894758Z","iopub.execute_input":"2024-07-17T15:07:45.895075Z","iopub.status.idle":"2024-07-17T15:07:45.902384Z","shell.execute_reply.started":"2024-07-17T15:07:45.895045Z","shell.execute_reply":"2024-07-17T15:07:45.901541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#### 2. train / test split\n\n# val_size = 0.2\n# indices = list(range(len(df)))\n\n# np.random.shuffle(indices)\n# split = int(np.floor(val_size * len(df)))\n# train_indices, val_indices = indices[split:], indices[:split]\n\n# train_ds = Subset(faceds, train_indices)\n# val_ds = Subset(faceds, val_indices)\n\n# train_loader = DataLoader(train_ds, batch_size=32, shuffle = True)\n# val_loader = DataLoader(val_ds, batch_size=32, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:45.903538Z","iopub.execute_input":"2024-07-17T15:07:45.904413Z","iopub.status.idle":"2024-07-17T15:07:45.911481Z","shell.execute_reply.started":"2024-07-17T15:07:45.904381Z","shell.execute_reply":"2024-07-17T15:07:45.910709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using the keypoint detection model from pytorch\nhttps://pytorch.org/vision/stable/models.html#keypoint-detection","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.models.detection import keypointrcnn_resnet50_fpn, KeypointRCNN_ResNet50_FPN_Weights\nfrom torchvision.io import read_image\n\nperson_int = read_image(IMAGE_DIR+df.iloc[8,0])\n\nweights = KeypointRCNN_ResNet50_FPN_Weights.DEFAULT\ntransforms = weights.transforms()\n\nperson_float = transforms(person_int)\n\nmodel = keypointrcnn_resnet50_fpn(weights=weights, progress=False)\nmodel = model.eval()\n\noutputs = model([person_float])\nprint(outputs)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:45.91244Z","iopub.execute_input":"2024-07-17T15:07:45.912743Z","iopub.status.idle":"2024-07-17T15:07:51.422367Z","shell.execute_reply.started":"2024-07-17T15:07:45.912711Z","shell.execute_reply":"2024-07-17T15:07:51.421428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kpts = outputs[0]['keypoints']\nscores = outputs[0]['scores']\n\nprint(kpts)\nprint(scores)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:51.423576Z","iopub.execute_input":"2024-07-17T15:07:51.423941Z","iopub.status.idle":"2024-07-17T15:07:51.43131Z","shell.execute_reply.started":"2024-07-17T15:07:51.423915Z","shell.execute_reply":"2024-07-17T15:07:51.430428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms.functional as F\n\ndef show(imgs):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:51.432553Z","iopub.execute_input":"2024-07-17T15:07:51.43291Z","iopub.status.idle":"2024-07-17T15:07:51.444419Z","shell.execute_reply.started":"2024-07-17T15:07:51.432879Z","shell.execute_reply":"2024-07-17T15:07:51.443628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detect_threshold = 0.75\nidx = torch.where(scores > detect_threshold)\nkeypoints = kpts[idx]","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:51.445526Z","iopub.execute_input":"2024-07-17T15:07:51.445934Z","iopub.status.idle":"2024-07-17T15:07:51.454839Z","shell.execute_reply.started":"2024-07-17T15:07:51.445904Z","shell.execute_reply":"2024-07-17T15:07:51.453994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.utils import draw_keypoints\n\nres = draw_keypoints(person_int, keypoints, colors=\"blue\",radius=3)\nshow(res)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:51.459894Z","iopub.execute_input":"2024-07-17T15:07:51.460238Z","iopub.status.idle":"2024-07-17T15:07:51.696682Z","shell.execute_reply.started":"2024-07-17T15:07:51.460215Z","shell.execute_reply":"2024-07-17T15:07:51.695855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dir(model)\n# model.state_dict\n# model","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:51.69792Z","iopub.execute_input":"2024-07-17T15:07:51.698532Z","iopub.status.idle":"2024-07-17T15:07:51.702141Z","shell.execute_reply.started":"2024-07-17T15:07:51.698502Z","shell.execute_reply":"2024-07-17T15:07:51.701298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print a summary using torchinfo (uncomment for actual output)\nsummary(model=model, \n        input_size=(32, 3,128,128), # make sure this is \"input_size\", not \"input_shape\"\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n) ","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:51.70321Z","iopub.execute_input":"2024-07-17T15:07:51.703494Z","iopub.status.idle":"2024-07-17T15:07:54.315587Z","shell.execute_reply.started":"2024-07-17T15:07:51.703471Z","shell.execute_reply":"2024-07-17T15:07:54.314708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use pretrained model on transformers ViT, and finetune to do image classification","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:07:54.316854Z","iopub.execute_input":"2024-07-17T15:07:54.317201Z","iopub.status.idle":"2024-07-17T15:08:08.168312Z","shell.execute_reply.started":"2024-07-17T15:07:54.31717Z","shell.execute_reply":"2024-07-17T15:08:08.167193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom transformers import AutoImageProcessor, ViTForImageClassification, ViTFeatureExtractor\n\nmodelVit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n# feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\nimage_processor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224')\n\n\n# Step 2: Modify the Model Architecture\nclass ModifiedViT(nn.Module):\n    def __init__(self, base_model):\n        super(ModifiedViT, self).__init__()\n        self.base_model = base_model\n        self.fc1 = nn.Linear(base_model.config.hidden_size, 256)\n        self.dropout1 = nn.Dropout(0.5)\n        self.classifier = nn.Linear(256, 1)\n\n    def forward(self, x):\n        x = self.base_model.vit(x).last_hidden_state[:, 0]\n        x = self.fc1(x)\n        x = nn.ReLU()(x)\n        x = self.dropout1(x)\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:08:08.169953Z","iopub.execute_input":"2024-07-17T15:08:08.170268Z","iopub.status.idle":"2024-07-17T15:08:22.806324Z","shell.execute_reply.started":"2024-07-17T15:08:08.170237Z","shell.execute_reply":"2024-07-17T15:08:22.805436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Make full use of GPUs Aavailable, since we have 2 T4 to use.\n\nmodifiedVit = ModifiedViT(modelVit)\n\nprint(\"Number of GPUs available:\", torch.cuda.device_count())\n\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n    ModifiedViT = nn.DataParallel(ModifiedViT)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:08:22.80747Z","iopub.execute_input":"2024-07-17T15:08:22.808013Z","iopub.status.idle":"2024-07-17T15:08:22.817379Z","shell.execute_reply.started":"2024-07-17T15:08:22.807986Z","shell.execute_reply":"2024-07-17T15:08:22.816525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## freeze the parameters of pretrained ViT\n\nmodifiedVit.base_model.classifier = None\n\nfor param in modifiedVit.base_model.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:08:22.818941Z","iopub.execute_input":"2024-07-17T15:08:22.81929Z","iopub.status.idle":"2024-07-17T15:08:22.845999Z","shell.execute_reply.started":"2024-07-17T15:08:22.81925Z","shell.execute_reply":"2024-07-17T15:08:22.84522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modifiedVit\nsummary(modifiedVit)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:08:22.847075Z","iopub.execute_input":"2024-07-17T15:08:22.848115Z","iopub.status.idle":"2024-07-17T15:08:22.915592Z","shell.execute_reply.started":"2024-07-17T15:08:22.848082Z","shell.execute_reply":"2024-07-17T15:08:22.914804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_processor","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:08:22.916811Z","iopub.execute_input":"2024-07-17T15:08:22.917346Z","iopub.status.idle":"2024-07-17T15:08:22.922991Z","shell.execute_reply.started":"2024-07-17T15:08:22.917316Z","shell.execute_reply":"2024-07-17T15:08:22.922089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## construct transformation for images\n\nfrom torchvision.transforms import (\n    CenterCrop,\n    Compose,\n    Normalize,\n    RandomHorizontalFlip,\n    RandomResizedCrop,\n    Resize,\n    ToTensor,\n)\n\nnormalize =Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\nif \"height\" in image_processor.size:\n    size=(image_processor.size[\"height\"],image_processor.size[\"width\"])\n    \n    crop_size = size\n    max_size =None\n    \n\ntrain_transforms = Compose(\n    [\n        RandomResizedCrop(crop_size),\n        RandomHorizontalFlip(p=0.4),\n        ToTensor(),\n        normalize,\n    ]\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:08:22.924072Z","iopub.execute_input":"2024-07-17T15:08:22.924316Z","iopub.status.idle":"2024-07-17T15:08:22.933458Z","shell.execute_reply.started":"2024-07-17T15:08:22.924295Z","shell.execute_reply":"2024-07-17T15:08:22.932719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_transforms = Compose(\n    [\n        Resize(size),\n        CenterCrop(crop_size),\n        ToTensor(),\n        normalize,\n    ]\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:08:22.934439Z","iopub.execute_input":"2024-07-17T15:08:22.934742Z","iopub.status.idle":"2024-07-17T15:08:22.948186Z","shell.execute_reply.started":"2024-07-17T15:08:22.93471Z","shell.execute_reply":"2024-07-17T15:08:22.947452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create train and test dataset and dataloader \n\n\nval_size = 0.2\nindices = list(range(len(df)))\n\nnp.random.shuffle(indices)\nsplit = int(np.floor(val_size * len(df)))\ntrain_indices, val_indices = indices[split:], indices[:split]\n\n\n# train_ds = Subset(faceds, train_indices)\n# val_ds = Subset(faceds, val_indices)\n\ntrain_ds = FaceData(df.iloc[train_indices,:], f'{BASE_DIR}/Images/Images', transform = train_transforms)\nval_ds = FaceData(df.iloc[val_indices,:], f'{BASE_DIR}/Images/Images', transform = val_transforms)\n\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle = True)\nval_loader = DataLoader(val_ds, batch_size=32, shuffle = True, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:08:22.949161Z","iopub.execute_input":"2024-07-17T15:08:22.949433Z","iopub.status.idle":"2024-07-17T15:08:22.963056Z","shell.execute_reply.started":"2024-07-17T15:08:22.949412Z","shell.execute_reply":"2024-07-17T15:08:22.962247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(df.iloc[train_indices,:])\nprint( f'{BASE_DIR}/Images/Images')","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:08:22.96437Z","iopub.execute_input":"2024-07-17T15:08:22.964699Z","iopub.status.idle":"2024-07-17T15:08:22.975294Z","shell.execute_reply.started":"2024-07-17T15:08:22.964657Z","shell.execute_reply":"2024-07-17T15:08:22.974403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Use weight and bias to monitor and record the experiments\ntry:\n    import wandb\nexcept:\n    print(\"[INFO] Couldn't find wandb, ... installing it.\")\n    !pip install wandb\n    import wandb\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:08:22.976457Z","iopub.execute_input":"2024-07-17T15:08:22.976747Z","iopub.status.idle":"2024-07-17T15:08:23.559349Z","shell.execute_reply.started":"2024-07-17T15:08:22.976722Z","shell.execute_reply":"2024-07-17T15:08:23.558617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nkaggle_secret = user_secrets.get_secret(\"wandb_api_key\") \nwandb.login(key=kaggle_secret)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:08:23.560567Z","iopub.execute_input":"2024-07-17T15:08:23.561338Z","iopub.status.idle":"2024-07-17T15:08:26.203334Z","shell.execute_reply.started":"2024-07-17T15:08:23.561304Z","shell.execute_reply":"2024-07-17T15:08:26.202332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import wandb\n# import random\n\n# # start a new wandb run to track this script\n# wandb.init(\n#     # set the wandb project where this run will be logged\n#     project=\"facial_beauty\",\n#     settings=wandb.Settings(start_method=\"fork\"),\n# #     id =\"facial-240706\",\n# #     resume = \"must\",\n# )","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:08:26.204455Z","iopub.execute_input":"2024-07-17T15:08:26.205022Z","iopub.status.idle":"2024-07-17T15:08:26.209436Z","shell.execute_reply.started":"2024-07-17T15:08:26.204996Z","shell.execute_reply":"2024-07-17T15:08:26.208344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Log hyperparameters\n# wandb.config = {\n#     \"learning_rate\": 0.001,\n#     \"epochs\": 5,\n#     \"batch_size\": 32\n# }\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:08:26.210628Z","iopub.execute_input":"2024-07-17T15:08:26.210924Z","iopub.status.idle":"2024-07-17T15:08:26.224138Z","shell.execute_reply.started":"2024-07-17T15:08:26.2109Z","shell.execute_reply":"2024-07-17T15:08:26.223316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define hyperparameter space\n\nepochs_options = [5, 8]\nlearning_rate_options = [0.0005]\noptimizer_options = ['adam', 'sgd']","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:08:26.225164Z","iopub.execute_input":"2024-07-17T15:08:26.22547Z","iopub.status.idle":"2024-07-17T15:08:26.234065Z","shell.execute_reply.started":"2024-07-17T15:08:26.225439Z","shell.execute_reply":"2024-07-17T15:08:26.233213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def initialize_model():\n    modelVit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n    modifiedVit_ins = ModifiedViT(modelVit)\n\n    print(\"Number of GPUs available:\", torch.cuda.device_count())\n\n    if torch.cuda.device_count() > 1:\n        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n        modifiedViT_ins = nn.DataParallel(modifiedViT_ins)\n    ## freeze the parameters of pretrained ViT\n\n    modifiedVit_ins.base_model.classifier = None\n\n    for param in modifiedVit_ins.base_model.parameters():\n        param.requires_grad = False\n        \n    print(summary(modifiedVit_ins))\n    \n    return modifiedVit_ins","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:08:26.235027Z","iopub.execute_input":"2024-07-17T15:08:26.235352Z","iopub.status.idle":"2024-07-17T15:08:26.24459Z","shell.execute_reply.started":"2024-07-17T15:08:26.235323Z","shell.execute_reply":"2024-07-17T15:08:26.243838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 4: Fine-Tune the Model\n\nfrom datetime import datetime\n\ndef train_model(num_epochs, learning_rate, optimizer):\n    # Start a new wandb run for each combination\n    wandb.init(project=\"facial_beauty_\" + datetime.now().strftime(\"%Y%m%d\"), id=f\"config_{num_epochs}_{optimizer}_\"+datetime.now().strftime(\"%H%M%S\"),\n                config={\"epochs\": num_epochs, \"optimizer\": optimizer})\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            \n    criterion = nn.MSELoss()\n    modifiedVit1 = initialize_model()\n    modifiedVit1.to(device)\n    if optimizer=='adam':\n        optimizer = optim.Adam(modifiedVit1.parameters(), lr=learning_rate)\n    else:\n        optimizer = optim.SGD(modifiedVit1.parameters(), lr=learning_rate)\n        \n    for epoch in range(num_epochs):\n        modifiedVit1.train()\n        running_loss = 0.0\n        for images, labels, _ in train_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = modifiedVit1(images)\n            loss = criterion(outputs.squeeze(), labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * images.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}')\n\n        # Log the loss\n        wandb.log({\"train_loss\": running_loss / len(train_loader.dataset)})\n\n\n        modifiedVit1.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for images, labels, _ in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = modifiedVit1(images)\n                loss = criterion(outputs.squeeze(), labels)\n                val_loss += loss.item() * images.size(0)\n\n        val_epoch_loss = val_loss / len(val_loader.dataset)\n        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_epoch_loss:.4f}')\n        # Log the loss\n        wandb.log({\"val_loss\": val_epoch_loss})\n        \n        # Finish the run\n    wandb.finish()\n    return val_epoch_loss, modifiedVit1\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:26:25.267895Z","iopub.execute_input":"2024-07-17T15:26:25.268622Z","iopub.status.idle":"2024-07-17T15:26:25.282872Z","shell.execute_reply.started":"2024-07-17T15:26:25.268592Z","shell.execute_reply":"2024-07-17T15:26:25.281723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"best_val_loss = float('inf')\nmodel_save_path = '/kaggle/working/best_model.pth'\n\nfor epochs in epochs_options:\n    for learning_rate in learning_rate_options:\n        for optimizer_choice in optimizer_options:\n            \n            # Train the model\n            val_ind_loss, cur_model = train_model(epochs, learning_rate, optimizer_choice)\n            # Check if this is the best model so far\n            if val_ind_loss < best_val_loss:\n                best_val_loss = val_ind_loss\n                # Save the model\n                torch.save(cur_model.state_dict(), model_save_path)\n                print(f'New best model saved with loss {val_ind_loss}, congfig: {epochs}{optimizer_choice}')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:26:30.6933Z","iopub.execute_input":"2024-07-17T15:26:30.693688Z","iopub.status.idle":"2024-07-17T15:51:02.657824Z","shell.execute_reply.started":"2024-07-17T15:26:30.693645Z","shell.execute_reply":"2024-07-17T15:51:02.657076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Best model: epoch 5 with Adam optimizer\n","metadata":{}},{"cell_type":"code","source":"## load the saved best model and test 1 photo\n\n# modifiedVit_final = ModifiedViT(modelVit)\n# modifiedVit_final.load_state_dict(torch.load(model_save_path))\n\nmodifiedVit_final = cur_model\n\n\n# Set the model to evaluation mode\nmodifiedVit_final.eval()\n\nindex = 0\n# Get the image and label\nimages, labels, _ = next(iter(val_loader))\nimage = images[index]\nlabel = labels[index]\n\n# Add a batch dimension (required for the model)\nimage = image.unsqueeze(0)\n\n# Move the image and the model to the same device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimage = image.to(device)\nmodel = model.to(device)\n\n# Make the prediction\nwith torch.no_grad():\n    output = modifiedVit_final(image)\n# If the output is a tensor, convert it to a list/float\npredicted_value = output.item()\n\n# Display the image and the prediction\nimage = image.squeeze().cpu().numpy().transpose((1, 2, 0))\nplt.imshow(image)\nplt.title(f\"Predicted: {predicted_value:.4f}, Actual: {label.item():.4f}\")\nplt.axis('off')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:56:20.904664Z","iopub.execute_input":"2024-07-17T15:56:20.905337Z","iopub.status.idle":"2024-07-17T15:56:21.277904Z","shell.execute_reply.started":"2024-07-17T15:56:20.905305Z","shell.execute_reply":"2024-07-17T15:56:21.277013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculate the predictions for the validation set, and plot the most mistaken predictions, dig out reasons.","metadata":{}},{"cell_type":"code","source":"val_pred_lst1, val_pred_lst2, val_pred_lst3 = [],[],[]\n\n\nfor i in tqdm(range(len(val_loader))):\n    images, labels, paths = next(iter(val_loader))\n#     print(labels)\n#     print(paths)\n\n    # Move the image and the model to the same device (CPU or GPU)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    images = images.to(device)\n    model = model.to(device)\n\n    # Make the prediction\n    with torch.no_grad():\n        outputs = modifiedVit_final(images)\n\n        outputs = torch.transpose(outputs, 0,1)\n#         print(outputs)\n\n        outputs = outputs.squeeze().to('cpu')\n    val_pred_lst1 += list(paths)\n    val_pred_lst2 += labels.tolist()\n    val_pred_lst3 += outputs.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:56:52.878113Z","iopub.execute_input":"2024-07-17T15:56:52.878483Z","iopub.status.idle":"2024-07-17T15:57:03.492989Z","shell.execute_reply.started":"2024-07-17T15:56:52.878453Z","shell.execute_reply":"2024-07-17T15:57:03.49204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_pred = pd.DataFrame({\n    'img_path': val_pred_lst1,\n    'label': val_pred_lst2,\n    'pred': val_pred_lst3\n})","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:57:05.628246Z","iopub.execute_input":"2024-07-17T15:57:05.6289Z","iopub.status.idle":"2024-07-17T15:57:05.634305Z","shell.execute_reply.started":"2024-07-17T15:57:05.628868Z","shell.execute_reply":"2024-07-17T15:57:05.633399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_pred.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:57:08.197835Z","iopub.execute_input":"2024-07-17T15:57:08.198196Z","iopub.status.idle":"2024-07-17T15:57:08.219341Z","shell.execute_reply.started":"2024-07-17T15:57:08.198168Z","shell.execute_reply":"2024-07-17T15:57:08.218486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_by_err(df, num_plots):\n    # Calculate the absolute difference between Column2 and Column3\n    df['AbsDiff'] = (df['label'] - df['pred']).abs()\n\n    # Sort the DataFrame by the absolute difference in descending order\n    df_sorted = df.sort_values(by='AbsDiff', ascending=False)\n\n    # Select the top rows\n    top_ = df_sorted.head(num_plots)\n    print(top_)\n    \n    fig = plt.figure(figsize=(9,9))\n    rows, cols = math.ceil(num_plots/3) ,3\n#     torch.manual_seed(random_seed)\n    for i in range(num_plots):\n        top_.iloc[i].values\n        img_path, label, pred,_ =  top_.iloc[i].values\n        fig.add_subplot(rows, cols, i+1)\n        im = Image.open(img_path)\n        plt.imshow(im)\n        plt.title(f'label:{label:.4f}|pred:{pred:.4f}')\n        plt.axis(False)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:57:17.219788Z","iopub.execute_input":"2024-07-17T15:57:17.220759Z","iopub.status.idle":"2024-07-17T15:57:17.228273Z","shell.execute_reply.started":"2024-07-17T15:57:17.220716Z","shell.execute_reply":"2024-07-17T15:57:17.227407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_by_err(val_pred, 18)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:57:19.757117Z","iopub.execute_input":"2024-07-17T15:57:19.757747Z","iopub.status.idle":"2024-07-17T15:57:21.29011Z","shell.execute_reply.started":"2024-07-17T15:57:19.757708Z","shell.execute_reply":"2024-07-17T15:57:21.289202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Widget to predict an arbitrary photo!","metadata":{}},{"cell_type":"code","source":"\nimport torch\nfrom torchvision import transforms, models\nfrom IPython.display import display, clear_output\nfrom ipywidgets import widgets, VBox, Output\nfrom PIL import Image\nimport io\nimport matplotlib.pyplot as plt\n\n\ndef preprocess_image(image):\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    image = Image.open(io.BytesIO(image)).convert(\"RGB\")\n    image = transform(image).unsqueeze(0)\n    return image.to(device)\n\ndef predict_beauty_score(model, image):\n    image = preprocess_image(image)\n    model.eval()\n    with torch.no_grad():\n        output = model(image)\n        score = output.item()\n    return score\n\ndef upload_and_predict():\n    uploader = widgets.FileUpload(accept='image/*', multiple=False)\n    result_button = widgets.Button(description=\"Result:\")\n    output = Output()\n\n    def on_button_click(b):\n        for filename in uploader.value:\n            content = uploader.value[filename]['content']\n            uploaded_image = content\n            score = predict_beauty_score(modifiedVit_final,uploaded_image)\n            img = Image.open(io.BytesIO(uploaded_image))\n\n            with output:\n                clear_output(wait=True)\n                plt.figure(figsize=(6, 6))\n                plt.imshow(img)\n                plt.title(f'Beauty Score: {score:.2f}')\n                plt.axis('off')\n                plt.show()\n\n    result_button.on_click(on_button_click)\n    display(VBox([uploader, result_button, output]))\n\nupload_and_predict()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T15:59:31.947142Z","iopub.execute_input":"2024-07-17T15:59:31.9478Z","iopub.status.idle":"2024-07-17T15:59:31.971397Z","shell.execute_reply.started":"2024-07-17T15:59:31.947768Z","shell.execute_reply":"2024-07-17T15:59:31.970554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}